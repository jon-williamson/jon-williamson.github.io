---
layout: page
title: Objective Bayesianism
permalink: /ob/index.html
exclude: true
---

My account of objective Bayesianism is epistemological: a theory of rational degree of belief, rather than a theory of statistical inference.

The account is distinctive in that:

*   It rejects the usual Bayesian identification of conditional degree of belief with conditional probability.
*   It doesn’t presuppose that degrees of belief should be updated by Bayesian conditionalisation.
*   It rejects a common objective Bayesian assumption that evidence uniquely determines a rational belief function.
*   It takes objective chances to play a central role in determining rational degrees of belief.

The account is built on three kinds of norm:

*   **Structural**. An agent’s belief function should be a probability function.
*   **Evidential**. If the agent establishes from evidence that the chance function is in some set of probability functions, then her belief function should be in the convex hull of that set.
*   **Equivocation**. The agent’s degrees of belief should otherwise be equivocal, adopting committal degrees of belief (near 0 or 1) only where they are forced by structural or evidential norms.

<p style="text-align: center;">*</p>

**Motivation**


<a href="https://global.oup.com/academic/product/in-defence-of-objective-bayesianism-9780199228003"><img style="float: right;" src="/publications/images/idoob.jpg" alt="" width="199"  /></a>For some recent arguments for this sort of approach, see:

<span style="color: #800000">Jon Williamson: </span><strong><a title="Bayesian philosophy and Bayesian statistics have diverged in recent years, because Bayesian philosophers have become more interested in philosophical problems other than the foundations of statistics and Bayesian statisticians have become less concerned with philosophical foundations. One way in which this divergence manifests itself is through the use of direct inference principles: Bayesian philosophers routinely advocate principles that require calibration of degrees of belief to available non-epistemic probabilities, while Bayesian statisticians rarely invoke such principles. As I explain, however, the standard Bayesian framework cannot coherently employ direct inference principles. Direct inference requires a shift towards a non-standard Bayesian framework, which further increases the gap between Bayesian philosophy and Bayesian statistics. This divergence does not preclude the application of Bayesian philosophical methods to real-world problems. Data consolidation is a key challenge for present-day systems medicine and other systems sciences. I show that data consolidation requires direct inference and that the non-standard Bayesian methods outlined here are well suited to this task.">Bayesianism from a philosophical perspective and its application to medicine</a></strong>, <em>International Journal of Biostatistics </em>19(2): 295-307, 2023. <a href="https://www.degruyter.com/document/doi/10.1515/ijb-2022-0043/pdf"><img class="alignnone" src="/images/openaccess.jpg" alt="" width="44" height="16" border="0" /><img class="alignnone" src="/images/acrobat.gif" alt="" width="16" height="16" border="0" /></a> doi: <a href="http://doi.org/10.1515/ijb-2022-0043"><span class="" data-seleniumid="article-doi-text">10.1515/ijb-2022-0043</span></a>

<span style="color: #800000">Jon Williamson: </span><strong><a title="When a proposition is established, it can be taken as evidence for other propositions. Can the Bayesian theory of rational belief and action provide an account of establishing? I argue that it can, but only if the Bayesian is willing to endorse objective constraints on both probabilities and utilities, and willing to deny that it is rationally permissible to defer wholesale to expert opinion. I develop a new account of deference that accommodates this latter requirement.">A Bayesian account of establishing</a></strong>, <em>British Journal for the Philosophy of Science</em> 73(4):903-925, 2022. <a href="https://www.journals.uchicago.edu/doi/pdf/10.1086/714798"><img class="alignnone" src="/images/openaccess.jpg" alt="" width="44" height="16" border="0" /><img class="alignnone" src="/images/acrobat.gif" alt="" width="16" height="16" border="0" /></a> <a href="/documents/BayesianEstablishing.pdf"><img id="IMG47" class="alignnone" src="/images/acrobat.gif" alt="" width="16" height="16" border="0" /></a> . doi: <a href="https://doi.org/10.1086/714798">10.1086/714798</a>


<span style="color: #993300">Jon Williamson</span>: <strong><a title="Schurz (2019, Chapter 4) argues that probabilistic accounts of induction fail. In particular, he criticises probabilistic accounts of induction that appeal to direct inference principles, including subjective Bayesian approaches (e.g., Howson, 2000) and objective Bayesian approaches (see, e.g., Williamson, 2017). In this paper, I argue that Schurz’ preferred direct inference principle, namely Reichenbach’s Principle of the Narrowest Reference Class, faces formidable problems in a standard probabilistic setting. Furthermore, the main alternative direct inference principle, Lewis’ Principal Principle, is also hard to reconcile with standard probabilism. So, I argue, standard probabilistic approaches cannot appeal to direct inference to explicate the logic of induction. However, I go on to defend a non-standard objective Bayesian account of induction: I argue that this approach can both accommodate direct inference and provide a viable account of the logic of induction. I then defend this account against Schurz’ criticisms.">Direct inference and probabilistic accounts of induction</a></strong>, <em>Journal for General Philosophy of Science</em> <span dir="ltr" role="presentation"> 54:451–472</span>, 2023. <a href="https://link.springer.com/content/pdf/10.1007/s10838-021-09584-0.pdf"><img class="alignnone" src="/images/openaccess.jpg" alt="" width="44" height="16" border="0" /><img class="alignnone" src="/images/acrobat.gif" alt="" width="16" height="16" border="0" /></a> <a href="/documents/ProbabilisticInduction.pdf"><img id="IMG47" class="alignnone" src="/images/acrobat.gif" alt="" width="16" height="16" border="0" /></a> . doi: <a href="https://doi.org/10.1007/s10838-021-09584-0">10.1007/s10838-021-09584-0</a>

For an introduction to the approach, see:

Jon Williamson: [**In defence of objective Bayesianism**](https://ukcatalogue.oup.com/product/9780199228003.do), Oxford University Press, 2010.

<p style="text-align: center;">*</p>

**Objective Bayesian inductive logic**

<a href="https://global.oup.com/academic/product/lectures-on-inductive-logic-9780199666478"><img style="float: right;" src="/publications/images/LOIL.jpg" alt="" width="199"  /></a>I’m also interested in the use of objective Bayesianism to provide a new approach to inductive logic. I’m currently collaborating with [Juergen Landes](https://jlandes.wordpress.com/) and [Soroush Rafiee Rad](http://rafieerad.org/) on this.

Recent work includes:

<span style="color: #993300">Jon Williamson</span>: <strong><a title="Edwin Jaynes’ principle of maximum entropy holds that one should use the probability distribution with maximum entropy, from all those that fit the evidence, to draw inferences, because that is the distribution that is maximally non-committal with respect to propositions that are underdetermined by the evidence. The principle was widely applied in the years following its introduction in 1957, and in 1978 Jaynes took stock, writing the paper ‘Where do we stand on maximum entropy?’ to present his view of the state of the art. Jaynes’ principle needs to be generalised to a principle of maximal entropy if it is to be applied to first-order inductive logic, where there may be no unique maximum entropy function. The development of this objective Bayesian inductive logic has also been very fertile and it is the task of this chapter to take stock. The chapter provides an introduction to the logic and its motivation, explaining how it overcomes some problems with Carnap’s approach to inductive logic and with the subjective Bayesian approach. It also describes a range of recent results that shed light on features of the logic, its robustness and its decidability, as well as methods for performing inference in the logic.">Where do we stand on maximal entropy? </a></strong>In <em>Logic for data, </em>eds Hykel Hosni &amp; Juergen Landes, Springer, 2024. <a href="/documents/MaximalEntropy.pdf"><img class="alignnone" src="/images/acrobat.gif" alt="" width="16" height="16" border="0" /></a>

<span style="color: #800000">Juergen Landes, Soroush Rafiee Rad and Jon Williamson: </span><strong><a title="According to the objective Bayesian approach to inductive logic, premisses inductively entail a conclusion just when every probability function with maximal entropy, from all those that satisfy the premisses, satisfies the conclusion. When premisses and conclusion are constraints on probabilities of sentences of a first-order predicate language, however, it is by no means obvious how to determine these maximal entropy functions. This paper makes progress on the problem in the following ways. Firstly, we introduce the concept of a limit in entropy and show that, if the set of probability functions satisfying the premisses contains a limit in entropy, then this limit point is unique and is the maximal entropy probability function. Next, we turn to the special case in which the premisses are categorical sentences of the logical language. We show that if the uniform probability function gives the premisses positive probability, then the maximal entropy function can be found by simply conditionalising this uniform prior on the premisses. We generalise our results to demonstrate agreement between the maximal entropy approach and Jeffrey conditionalisation in the case in which there is a single premiss that specifies the probability of a sentence of the language. We show that, after learning such a premiss, certain inferences are preserved, namely inferences to inductive tautologies. Finally, we consider potential pathologies of the approach: we explore the extent to which the maximal entropy approach is invariant under permutations of the constants of the language, and we discuss some cases in which there is no maximal entropy probability function.">Determining maximal entropy functions for objective Bayesian inductive logic</a></strong>, <em>Journal of Philosophical Logic</em> 52:555-608, 2023.  <a href="https://link.springer.com/content/pdf/10.1007/s10992-022-09680-6.pdf"><img class="alignnone" src="/images/openaccess.jpg" alt="" width="44" height="16" border="0" /><img class="alignnone" src="/images/acrobat.gif" alt="" width="16" height="16" border="0" /></a> doi: <a href="https://doi.org/10.1007/s10992-022-09680-6">10.1007/s10992-022-09680-6</a>

<p>Juergen Landes, Soroush Rafiee Rad and Jon Williamson: <strong><a title="The maximum entropy principle is widely used to determine non-committal probabilities on a finite domain, subject to a set of constraints, but its application to continuous domains is notoriously problematic. This paper concerns an intermediate case, where the domain is a first-order predicate language. Two strategies have been put forward for applying the maximum entropy principle on such a domain: (i) applying it to finite sublanguages and taking the pointwise limit of the resulting probabilities as the size n of the sublanguage increases; (ii) selecting a probability function on the language as a whole whose entropy on finite sublanguages of size n is not dominated by that of any other probability function for sufficiently large n. The entropy-limit conjecture says that, where these two approaches yield determinate probabilities, the two methods yield the same probabilities. If this conjecture is found to be true, it would provide a boost to the project of seeking a single canonical inductive logic—a project which faltered when Carnap’s attempts in this direction succeeded only in determining a continuum of inductive methods. The truth of the conjecture would also boost the project of providing a canonical characterisation of normal or default models of first-order theories. Hitherto, the entropy-limit conjecture has been verified for languages which contain only unary predicate symbols and also for the case in which the constraints can be captured by a categorical statement of Σ 1 quantifier complexity. This paper shows that the entropy-limit conjecture also holds for categorical statements of Π 1 complexity, for various non-categorical constraints, and in certain other general situations.">Towards the Entropy-Limit Conjecture</a></strong>, <em>Annals of Pure and Applied Logic</em> 172(2):102870, 2021. <a href="https://www.sciencedirect.com/science/article/pii/S0168007220300944/pdfft?isDTMRedir=true&amp;download=true"><img class="alignnone" src="../images/openaccess.jpg" alt="" width="44" height="16" border="0" /><img class="alignnone" src="../images/acrobat.gif" alt="" width="16" height="16" border="0" /></a> . doi: <a href="https://doi.org/10.1016/j.apal.2020.102870">10.1016/j.apal.2020.102870</a></p>

For an introduction to the approach, see:

Jon Williamson: [**Lectures on inductive logic**](https://global.oup.com/academic/product/lectures-on-inductive-logic-9780199666478?cc=gb&lang=en&), Oxford University Press, 2017. [Errata](/publications/LOIL-errata.html).

<p style="text-align: center;">*</p>

**Objective Bayesian nets**

<a href="https://global.oup.com/academic/product/lectures-on-inductive-logic-9780199666478"><img style="float: right;" src="/publications/images/bnac.jpg" alt="" width="199"  /></a>Graphical models can be used to represent and reason with objective Bayesian probabilities.

Recent work includes:

<span style="color: #800000">Juergen Landes and Jon Williamson: </span><strong><a title="This paper addresses a data integration problem: given several mutually consistent datasets each of which measures a subset of the variables of interest, how can one construct a probabilistic model that fits the data and gives reasonable answers to questions which are under-determined by the data? Here we show how to obtain a Bayesian network model which represents the unique probability function that agrees with the probability distributions measured by the datasets and otherwise has maximum entropy. We provide a general algorithm, OBN-cDS, which offers substantial efficiency savings over the standard brute-force approach to determining the maximum entropy probability function. Furthermore, we develop modifications to the general algorithm which enable further efficiency savings but which are only applicable in particular situations. We show that there are circumstances in which one can obtain the model (i) directly from the data; (ii) by solving algebraic problems; and (iii) by solving relatively simple independent optimisation problems.">Objective Bayesian nets for integrating consistent datasets</a></strong>, <em>Journal of Artificial Intelligence Research</em> 74: 393-458, 2022. <a href="https://jair.org/index.php/jair/article/view/13363/26805"><img class="alignnone" src="/images/openaccess.jpg" alt="" width="44" height="16" border="0" /><img class="alignnone" src="/images/acrobat.gif" alt="" width="16" height="16" border="0" /></a>. doi <a href="https://doi.org/10.1613/jair.1.13363">10.1613/jair.1.13363</a>

For an introduction, see:

Rolf Haenni, Jan-Willem Romeijn, Gregory Wheeler & Jon Williamson: **[Probabilistic logics and probabilistic networks](https://www.springer.com/philosophy/epistemology+and+philosophy+of+science/book/978-94-007-0007-9)**, [Synthese Library](https://www.springer.com/series/6607), Springer, 2011.

Jon Williamson: [**Bayesian nets and causality: philosophical and computational foundations**](https://ukcatalogue.oup.com/product/9780198530794.do), Oxford University Press, 2005.