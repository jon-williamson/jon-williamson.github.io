---
layout: page
title: Lectures on Inductive Logic errata
permalink: /pages/LOIL-errata.html
exclude: true
---

<a href="http://blogs.kent.ac.uk/jonw/files/2015/01/LOIL-1.jpg"><img style="float: right;"  src="http://blogs.kent.ac.uk/jonw/files/2015/01/LOIL-1.jpg" alt="" width="180" height="272" /></a> Here are some errata for my book <a href="https://global.oup.com/academic/product/lectures-on-inductive-logic-9780199666478?cc=gb&amp;lang=en&amp;"><strong>Lectures on inductive logic</strong></a>, Oxford University Press, 2017. I'd be very grateful if you could <a href="mailto:j.williamson@kent.ac.uk">let me know</a> of any other mistakes or typos.

p.8. line 2 from bottom should begin "it has".

p.26. The formula in definition 2.15 should begin "d_n(P,Q)".

p.97. The proofs of Theorems 5.16, 5.17 and 5.18 go through only in the case in which the evidence set is finitely generated. For proofs that apply even where the evidence set is not finitely generated, see:
<span style="color: #993300">Juergen Landes, Soroush Rafiee Rad and Jon Williamson</span>: <strong><a title="According to the objective Bayesian approach to inductive logic, premisses inductively entail a conclusion just when every probability function with maximal entropy, from all those that satisfy the premisses, satisfies the conclusion. When premisses and conclusion are constraints on probabilities of sentences of a first-order predicate language, however, it is by no means obvious how to determine these maximal entropy functions. This paper makes progress on the problem in the following ways. Firstly, we introduce the concept of a limit in entropy and show that, if the set of probability functions satisfying the premisses contains a limit in entropy, then this limit point is unique and is the maximal entropy probability function. Next, we turn to the special case in which the premisses are categorical sentences of the logical language. We show that if the uniform probability function gives the premisses positive probability, then the maximal entropy function can be found by simply conditionalising this uniform prior on the premisses. We generalise our results to demonstrate agreement between the maximal entropy approach and Jeffrey conditionalisation in the case in which there is a single premiss that specifies the probability of a sentence of the language. We show that, after learning such a premiss, certain inferences are preserved, namely inferences to inductive tautologies. Finally, we consider potential pathologies of the approach: we explore the extent to which the maximal entropy approach is invariant under permutations of the constants of the language, and we discuss some cases in which there is no maximal entropy probability function.">Determining maximal entropy functions for objective Bayesian inductive logic</a></strong>, <em>Journal of Philosophical Logic </em>52:555-608, 2023<em>. </em> <a href="https://link.springer.com/content/pdf/10.1007/s10992-022-09680-6.pdf"><img class="alignnone" src="../images/openaccess.jpg" alt="" width="44" height="16" border="0" /><img class="alignnone" src="../images/acrobat.gif" alt="" width="16" height="16" border="0" /></a> doi: <a href="https://doi.org/10.1007/s10992-022-09680-6">10.1007/s10992-022-09680-6</a>

p.129. The statement on line 1 does not hold for all theta, but does hold for the instances of theta provided subsequently.

p.182, line 6: "They key point" -&gt; "The key point".